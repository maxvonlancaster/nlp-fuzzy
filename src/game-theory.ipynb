{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b7b6e05",
   "metadata": {},
   "source": [
    "# Some general ideas related to game theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2515f5c6",
   "metadata": {},
   "source": [
    "## Poker Problem \n",
    "\n",
    "Kuhn Poker: 1,2,3 -> \n",
    "\n",
    "Two pleyer\n",
    "$$a_{}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab420b6c",
   "metadata": {},
   "source": [
    "## Adversarial Generation of Short Prompts for Improving Feedback Quality\n",
    "\n",
    "**Motivation**\n",
    "\n",
    "User feedback (reviews, evaluations, surveys) is often affected by cognitive biases, emotional tone, and low informational density. Respondents tend to provide vague, emotionally charged, or socially biased answers, which reduces the usefulness of feedback for decision-making and analysis. This problem is especially pronounced in educational evaluations, product reviews, and employee assessments.\n",
    "\n",
    "\n",
    "How can short textual prompts be adaptively generated to maximize the information content and reduce bias in customer feedback, under adversarial evaluation?\n",
    "\n",
    "feedback quality metrics\n",
    "- Information content I(r) (number of distinct product aspects mentioned, or entropy of aspect distribution, or coverage of predefined aspect set)\n",
    "- Bias metric B(r) (sentiment extremeness (absolute sentiment score), demographic skew (if groups exist), lexical subjectivity score.)\n",
    "- Utility U(r) (Human usefulness. Collect later via human ratings 1–5. At the beginning, can be ignored)\n",
    "\n",
    "GAME:\n",
    "Generator (G) produces a short prompt p, Evaluator (E) evaluates the resulting feedback r.\n",
    "G: finite set of prompt types (templates); E: fixed scoring function (at first)\n",
    "\n",
    "Payoff: $u_{G}​(p)=\\alpha I(r)−\\beta B(r)$\n",
    "Evaluator payoff: $uE​(p)=−uG​(p)$ -- this is for now a zero-sum game\n",
    "\n",
    "The game is between a Prompt Generator and an Evaluator of feedback quality, where the generator tries to elicit informative, low-bias feedback, and the evaluator penalizes bias and low information.\n",
    "\n",
    "BASELINE PIPELINE:\n",
    "- Create a small prompt set - pure strategies\n",
    "- Collect responses (may simulate using LLM)\n",
    "- NLP analysis: extract aspects (rule-based or simple classifier), compute sentiment score.\n",
    "\n",
    "payoff matrix\n",
    "\n",
    "Now: eliminate dominated prompts, compute mixed strategies, identify equilibrium distributions.\n",
    "\n",
    "FURTHER:\n",
    "adversarial learning\n",
    "- Parameterize the prompt - Instead of fixed templates, represent prompt as (length,specificity,number of constraints,presence of examples) -> vector $\\theta$\n",
    "- Train generator, using RL (action = generate prompt parameters, reward = $u_{G}$, environment = evaluator + NLP pipeline.)\n",
    "- \n",
    "\n",
    "comparison of prompts before and after optimization, statistical improvement in I and reduction in B, robustness analysis across datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06c31f1",
   "metadata": {},
   "source": [
    "## Adversarial Generation of Short Prompts for Improving Feedback Quality\n",
    "\n",
    "customer feedback in textual form\n",
    "\n",
    "$$R=\\{all possible feedback texts\\}$$\n",
    "\n",
    "$$P=\\{all possible short prompts\\}$$\n",
    "\n",
    "Given a prompt $p\\in P$, the user (environment) generates a feedback text:\n",
    "\n",
    "$$r∼P(⋅∣p),r\\in R$$\n",
    "\n",
    "The distribution is unknown and stochastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ffb4066",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "SECRET_KEY = os.getenv(\"OPEN_API_KEY\")\n",
    "\n",
    "PROMPTS = [\n",
    "    \"What did you like about the product?\",\n",
    "    \"What problems did you face while using the product?\",\n",
    "    \"Please describe your experience with the product.\",\n",
    "    \"What should we improve in the product?\",\n",
    "    \"How satisfied are you with the product and why?\"\n",
    "]\n",
    "\n",
    "client = OpenAI(api_key=SECRET_KEY)\n",
    "\n",
    "def generate_feedback(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a customer giving honest feedback.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.9,\n",
    "        max_tokens=120\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0a5779",
   "metadata": {},
   "source": [
    "Let $A=\\{ a_{1}, ...,a_{k} \\}$ be a predefined set of product aspects.\n",
    "\n",
    "Define an aspect extraction function: $\\phi :R\\rightarrow 2^{A}$\n",
    "\n",
    "Information content: $I(r)=∣\\phi (r)∣$\n",
    "\n",
    "Lexical divercity:\n",
    "\n",
    "$$I(r) = \\frac{unique tokens}{total tokens}$$\n",
    "\n",
    "DEFINE BETTER FUNCTION LATER!\n",
    "\n",
    "Bias metric:\n",
    "\n",
    "$$B(r)=|s(r)|$$\n",
    "\n",
    "Where \n",
    "$s(r)\\in [−1,1] $ - sentiment polarity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be7d863f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from textblob import TextBlob\n",
    "\n",
    "\n",
    "def information_metric(text):\n",
    "    tokens = re.findall(r\"\\w+\", text.lower())\n",
    "    if len(tokens) == 0:\n",
    "        return 0.0\n",
    "    return len(set(tokens)) / len(tokens)\n",
    "\n",
    "def bias_metric(text):\n",
    "    sentiment = TextBlob(text).sentiment.polarity\n",
    "    return abs(sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50573e7b",
   "metadata": {},
   "source": [
    "Empirical expectation (Monte Carlo)\n",
    "\n",
    "Since feedback is stochastic, we work with expectations.\n",
    "\n",
    "$$E[B∣p]=E_{r∼P(⋅∣p)}​[B(r)]$$\n",
    "$$E[I∣p]=E_{r∼P(⋅∣p)}​[I(r)]$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4e1ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def estimate_expectations(prompt, n_samples=10):\n",
    "    infos, biases = [], []\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        r = generate_feedback(prompt)\n",
    "        infos.append(information_metric(r))\n",
    "        biases.append(bias_metric(r))\n",
    "\n",
    "    return np.mean(infos), np.mean(biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b713d8ba",
   "metadata": {},
   "source": [
    "**Payoff function (parametrized)**\n",
    "\n",
    "Player G (Generator) chooses prompt p\n",
    "\n",
    "Player E (Evaluator) enforces quality constraints\n",
    "\n",
    "Strategy spaces\n",
    "\n",
    "Generator strategies:\n",
    "\n",
    "$p\\in P$\n",
    "\n",
    "Evaluator strategies:\n",
    "\n",
    "$$\\lambda = (\\alpha, \\beta)\\in \\mathbb{R}_{+}^{2}$$\n",
    "\t​\n",
    "Evaluator controls the penalty weights.\n",
    "\n",
    "Generator payoff:\n",
    "\n",
    "$$u_G​(p,λ)=αE[I∣p]−βE[B∣p]$$\n",
    "\n",
    "Evaluator payoff:\n",
    "\n",
    "$$u_E​(p,λ)=−u_G​(p,λ)$$\n",
    "\n",
    "**Static game with finite strategies:**\n",
    "\n",
    "Finite prompts: $P={p_1​,…,p_n​}$\n",
    "\n",
    "Define payoff: $U_i​=αE[I∣p_i​]−βE[B∣p_i​]$\n",
    "\n",
    "$p_{i}$ is dominated by $p_{j}$ if $Uj_​≥U_i​$. Dominated prompts can be eliminated.\n",
    "\n",
    "We seek a minimax equillibrium $\\max_{\\pi} \\min_{\\lambda} U_{G}(\\pi, \\lambda)$\n",
    "\n",
    "\n",
    "THINK ABOUT LATER: Dynamic learning formulation, \n",
    "Let prompts be generated by parameters: $p=g(\\theta),\\theta \\in \\mathbb{R}^{d}$\n",
    "\n",
    "Constraints on parameters: $\\alpha+\\beta=1$, $u_{E}​=−u_{G}​−c(\\beta)$ ($c(\\beta)=\\gamma \\beta^{2}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a3b063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def payoff(prompt, alpha=1.0, beta=1.0, n_samples=10):\n",
    "    I_hat, B_hat = estimate_expectations(prompt, n_samples)\n",
    "    return alpha * I_hat - beta * B_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c0f934",
   "metadata": {},
   "source": [
    "Playing the game (best response)\n",
    "\n",
    "The generator chooses the best prompt given evaluator parameters.\n",
    "\n",
    "THINK ABOUT LATER: Transformer as Prompt Generator\n",
    "\n",
    "Prompt as a function of parameters, instead of choosing a prompt directly, define: $p_{\\theta} =G_{\\theta}​(z)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "418b3303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best prompt: What problems did you face while using the product?\n",
      "What did you like about the product?... -> 0.5445\n",
      "What problems did you face while using t... -> 0.6946\n",
      "Please describe your experience with the... -> 0.6130\n",
      "What should we improve in the product?... -> 0.6290\n",
      "How satisfied are you with the product a... -> 0.5694\n",
      "beta=0.2 -> best prompt: What problems did you face while using the product?\n",
      "beta=0.5 -> best prompt: What problems did you face while using the product?\n",
      "beta=1.0 -> best prompt: What problems did you face while using the product?\n",
      "beta=2.0 -> best prompt: What problems did you face while using the product?\n"
     ]
    }
   ],
   "source": [
    "def best_prompt(prompts, alpha, beta):\n",
    "    scores = {}\n",
    "    for p in prompts:\n",
    "        scores[p] = payoff(p, alpha, beta)\n",
    "    return max(scores, key=scores.get), scores\n",
    "\n",
    "best, all_scores = best_prompt(PROMPTS, alpha=1.0, beta=0.7)\n",
    "\n",
    "print(\"Best prompt:\", best)\n",
    "for p, s in all_scores.items():\n",
    "    print(f\"{p[:40]}... -> {s:.4f}\")\n",
    "\n",
    "BETAS = [0.2, 0.5, 1.0, 2.0]\n",
    "\n",
    "for beta in BETAS:\n",
    "    best, _ = best_prompt(PROMPTS, alpha=1.0, beta=beta)\n",
    "    print(f\"beta={beta:.1f} -> best prompt: {best}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
