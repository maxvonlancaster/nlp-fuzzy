{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fce49dca",
   "metadata": {},
   "source": [
    "# Game-Theoretic Approach to NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9e1154",
   "metadata": {},
   "source": [
    "TBD: describe\n",
    "\n",
    "Reinforcement Learning with Human-in-the-Loop (RLHF) setup, with two agents:\n",
    "\n",
    "- Agent A: Text generator (e.g., a simple model or GPT-based).\n",
    "- Agent B: Evaluator that scores Agent Aâ€™s output based on a modifiable matrix.\n",
    "- Human: Oversees the process and can manually adjust the evaluation matrix used by B.\n",
    "\n",
    "This kind of framework could be used for dialogue training, text summarization, creative writing, or even value alignment experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4828ba1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 0 ---\n",
      "Generated: Tell me a story with extra words 2\n",
      "Reward: 0.45\n",
      "\n",
      "--- Step 1 ---\n",
      "Generated: Tell me a story with extra words 9\n",
      "Reward: 0.45\n",
      "\n",
      "--- Step 2 ---\n",
      "Generated: Tell me a story with extra words 1\n",
      "Reward: 0.45\n",
      "\n",
      "--- Step 3 ---\n",
      "Generated: Tell me a story with extra words 5\n",
      "Reward: 0.45\n",
      "\n",
      "--- Step 4 ---\n",
      "Generated: Tell me a story with extra words 6\n",
      "Reward: 0.45\n",
      "\n",
      "--- Step 5 ---\n",
      "Generated: Tell me a story with extra words 1\n",
      "Reward: 0.45\n",
      "\n",
      "--- Step 6 ---\n",
      "Generated: Tell me a story with extra words 7\n",
      "Reward: 0.45\n",
      "\n",
      "--- Step 7 ---\n",
      "Generated: Tell me a story with extra words 8\n",
      "Reward: 0.45\n",
      "\n",
      "--- Step 8 ---\n",
      "Generated: Tell me a story with extra words 2\n",
      "Reward: 0.45\n",
      "\n",
      "--- Step 9 ---\n",
      "Generated: Tell me a story with extra words 5\n",
      "Reward: 0.45\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# === Agent A: text generator (naive implementation) ===\n",
    "class AgentA:\n",
    "    def __init__(self):\n",
    "        self.temperature = 1.0  # will be tuned based on feedback\n",
    "\n",
    "    def generate(self, prompt):\n",
    "        # In a real case, this could be a language model\n",
    "        return f\"{prompt} with extra words {random.randint(0, int(10 * self.temperature))}\"\n",
    "\n",
    "    def update(self, reward):\n",
    "        # Simple feedback: increase \"risk\" (temperature) if rewarded\n",
    "        self.temperature += 0.1 * (reward - 0.5)  # normalize around 0.5\n",
    "        self.temperature = max(0.1, min(self.temperature, 2.0))\n",
    "\n",
    "\n",
    "# === Agent B: evaluator using a matrix ===\n",
    "class AgentB:\n",
    "    def __init__(self, matrix=None):\n",
    "        self.matrix = matrix or {\n",
    "            \"length_weight\": 1.0,\n",
    "            \"keyword_weight\": 1.0,\n",
    "            \"positivity_weight\": 1.0,\n",
    "        }\n",
    "\n",
    "    def evaluate(self, text):\n",
    "        score = 0\n",
    "        score += self.matrix[\"length_weight\"] * len(text.split())\n",
    "        score += self.matrix[\"keyword_weight\"] * (\"extra\" in text)\n",
    "        score += self.matrix[\"positivity_weight\"] * (\"good\" in text)\n",
    "        return min(score / 20.0, 1.0)  # normalize to [0,1]\n",
    "\n",
    "    def update_matrix(self, new_matrix):\n",
    "        self.matrix = new_matrix\n",
    "\n",
    "\n",
    "# === Human interface ===\n",
    "def human_adjust_matrix(matrix):\n",
    "    print(\"\\nCurrent matrix:\", matrix)\n",
    "    key = input(\"Change which weight (length/keyword/positivity)? Leave empty to skip: \").strip()\n",
    "    if key in matrix:\n",
    "        new_val = float(input(f\"New value for {key}: \"))\n",
    "        matrix[key] = new_val\n",
    "    return matrix\n",
    "\n",
    "\n",
    "# === Training loop ===\n",
    "def training_loop():\n",
    "    agent_a = AgentA()\n",
    "    agent_b = AgentB()\n",
    "    \n",
    "    for step in range(10):\n",
    "        print(f\"\\n--- Step {step} ---\")\n",
    "        prompt = \"Tell me a story\"\n",
    "        text = agent_a.generate(prompt)\n",
    "        reward = agent_b.evaluate(text)\n",
    "\n",
    "        print(f\"Generated: {text}\")\n",
    "        print(f\"Reward: {reward:.2f}\")\n",
    "\n",
    "        agent_a.update(reward)\n",
    "\n",
    "        user_input = input(\"Do you want to adjust evaluator? (y/n): \")\n",
    "        if user_input.lower().startswith('y'):\n",
    "            new_matrix = human_adjust_matrix(agent_b.matrix)\n",
    "            agent_b.update_matrix(new_matrix)\n",
    "\n",
    "training_loop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f08453d",
   "metadata": {},
   "source": [
    "## Where to start:\n",
    "- How to code an agent?\n",
    "- Vecotrization: 1,2 test data\n",
    "- Evaluation: binary cross-entropy \n",
    "- \n",
    "\n",
    "- Be more concentrated on context in dialogue "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e781fa4f",
   "metadata": {},
   "source": [
    "## General Idea:\n",
    "A, B \n",
    "\n",
    "B prompts A\n",
    "A answers B\n",
    "B evaluates A\n",
    "B prompts A (2)\n",
    "A ansers\n",
    "B evaluates prompt (1) and prompt (2)\n",
    "\n",
    "We have to have a matrix of context on the side of A\n",
    "\n",
    "Where game theory is?\n",
    "\n",
    "Human in the loop (H), H tracks interaction between A and B, and adjustes the matrix that B has;\n",
    "\n",
    "\n",
    "\n",
    "## Methodology\n",
    "\n",
    "- Trajection analysis (trajection: sequence of prompts and evaluations)\n",
    "- Direction alignment \n",
    "- Policy gradient algorithm\n",
    "- Rejection sampling\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecca0e43",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218be4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Agent Response: hello and Susan were two children now. It was all they could understand that night.\"\n",
      "\n",
      "She's probably won't be able to tell your child if it's \"The Best Day Ever\" because they probably will never have a real thought and not know\n",
      "Updated model with loss -0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Agent Response: hello, a bit on my part.\n",
      "\n",
      "How many people do you have at this office?\n",
      "\n",
      "Well, I started this week for the first time in my life. So I guess you didn't have the office?\n",
      "\n",
      "I didn't\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import numpy as np\n",
    "\n",
    "class RLTransformerAgent:\n",
    "    def __init__(self, model_name='gpt2', max_length=50, lr=1e-5):\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "        self.model.train()\n",
    "\n",
    "        self.max_length = max_length\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "\n",
    "    def generate_response(self, prompt, temperature=1.0):\n",
    "        inputs = self.tokenizer(prompt, return_tensors='pt')\n",
    "        output = self.model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=self.max_length,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "        )\n",
    "        response = self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        return response\n",
    "\n",
    "    def reinforce_update(self, prompt, response, reward):\n",
    "        inputs = self.tokenizer(prompt, return_tensors='pt')\n",
    "        full_input = self.tokenizer(prompt + response, return_tensors='pt')\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = self.model(**full_input, labels=full_input[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Policy gradient-style loss scaling\n",
    "        loss = loss * -reward  # Maximize reward\n",
    "\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "agent = RLTransformerAgent()\n",
    "\n",
    "for episode in range(10):\n",
    "    prompt = input(\"\\nEnter a prompt: \")\n",
    "    response = agent.generate_response(prompt)\n",
    "    print(f\"\\nAgent Response: {response}\")\n",
    "\n",
    "    reward = float(input(\"Enter reward (0.0 to 1.0): \"))\n",
    "    loss = agent.reinforce_update(prompt, response, reward)\n",
    "\n",
    "    print(f\"Updated model with loss {loss:.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
